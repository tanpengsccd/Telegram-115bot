# -*- coding: utf-8 -*-

import requests
import init
import time
from datetime import datetime
from sqlitelib import *
from bs4 import BeautifulSoup
from telegram import Bot
from message_queue import add_task_to_queue
import yaml
from download_handler import create_strm_file, notice_emby_scan_library


def get_actor_id(actor_name):
    # JavDB æ¼”å‘˜æœç´¢ URL
    search_url = f"https://javdb.com/search?q={actor_name}&f=actor"

    headers = {
        "User-Agent": init.USER_AGENT
    }

    try:
        # å‘èµ· GET è¯·æ±‚
        response = requests.get(search_url, headers=headers)
        response.raise_for_status()

        # ä½¿ç”¨ BeautifulSoup è§£æ HTML
        soup = BeautifulSoup(response.text, "html.parser")

        # æœç´¢æ‰€æœ‰æ¼”å‘˜é“¾æ¥ï¼Œé“¾æ¥ä¸­åŒ…å« "/actor/"
        actor_link = soup.find("a", 
                               href=lambda href: href and "/actors/" in href, 
                               title=lambda title: title and actor_name in title)

        # å¦‚æœæ‰¾åˆ°é“¾æ¥ï¼Œæå–ç¬¬ä¸€ä¸ªæ¼”å‘˜ ID
        if actor_link:
            actor_id = actor_link["href"].split("/")[-1]  # æå– ID
            return actor_id
        else:
            print("æœªæ‰¾åˆ°æ¼”å‘˜é“¾æ¥ï¼Œè¯·æ£€æŸ¥æ¼”å‘˜åå­—æˆ– HTML ç»“æ„ã€‚")
            return None

    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None


def del_all_subscribe():
    with SqlLiteLib() as sqlite:
        sql = f"delete from subscribe"
        sqlite.execute_sql(sql)
        init.logger.info("All subscribe has been deleted.")


def update_pub_url(number, pub_url):
    with SqlLiteLib() as sqlite:
        sql = f"update subscribe set pub_url=? where number=?"
        params = (pub_url, number)
        sqlite.execute_sql(sql, params)


def add_subscribe2db(actor_name, sub_user):
    actor_id = get_actor_id(actor_name)
    headers = {
        "user-agent": init.USER_AGENT,
        "cookie": init.JAVDB_COOKIE,
        "origin": "https://javdb.com"
    }
    base_url = f"https://javdb.com/actors/{actor_id}"
    max_pages = 10
    with SqlLiteLib() as sqlite:
        query = f"select number from subscribe where actor_id = ?"
        params = (actor_id,)
        res = sqlite.query(query, params)
        for page in range(1, max_pages + 1):
            url = f"{base_url}?page={page}&sort_type=0"
            init.logger.info(f"Get info from {url}")
            response = requests.get(url, headers=headers)

            if response.status_code != 200:
                init.logger.warn(f"Failed to fetch data for actor {actor_name}, page {page}")
                return ""

            soup = BeautifulSoup(response.text, features="html.parser")
            movie_list_div = soup.find('div', class_='movie-list h cols-4 vcols-8')
            if not movie_list_div:
                init.logger.info("No movies found or structure mismatch.")
                break

            item_divs = movie_list_div.findAll('div', class_='item')
            if not item_divs:
                init.logger.info("No items found in movie list.")
                break

            for item_div in item_divs:
                # æå–å›¾ç‰‡é“¾æ¥
                img_tag = item_div.find('img')
                post_url = img_tag['src'] if img_tag and 'src' in img_tag.attrs else ""

                # æå–æè¿°å¹¶è·³è¿‡ä¸éœ€è¦çš„é¡¹
                description = item_div.get_text()
                if "å«ä¸­å­—ç£éˆ" in description or "å«ç£éˆ" in description:
                    continue

                # æå–ç•ªå·
                video_title_div = item_div.find('div', class_='video-title')
                number = video_title_div.find('strong').text if video_title_div else "N/A"

                # æå–æ ‡é¢˜
                title = item_div.find('a', class_='box').get('title', "N/A")

                # æå–è¯„åˆ†
                score_div = item_div.find('div', class_='score')
                score_text = score_div.find('span', class_='value').text if score_div else "0 åˆ†"
                score = score_text.split('åˆ†')[0].strip()

                # æå–æ—¥æœŸ
                meta_div = item_div.find('div', class_='meta')
                pub_date = meta_div.text.strip() if meta_div else "æœªçŸ¥æ—¥æœŸ"

                if (number,) not in res:
                    # æ’å…¥æ•°æ®åˆ°æ•°æ®åº“
                    insert_sql = f'''INSERT INTO subscribe (actor_name, actor_id, number, pub_date, title, post_url, score, sub_user) VALUES (?,?,?,?,?,?,?,?)'''
                    init.logger.debug(insert_sql)
                    params = (actor_name, actor_id, number, pub_date, title, post_url, score, sub_user)
                    sqlite.execute_sql(insert_sql, params)
                    init.logger.info(f"[{number}] has been added to subscribe.")
                time.sleep(3)


def get_magnet_by_number(number):
    headers = {
        "user-agent": init.USER_AGENT,
        "cookie": init.JAVDB_COOKIE,
    }
    base_url = "https://javdb.com"
    url = f"{base_url}/search?q={number}&f=all"
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        init.logger.warn(f"Failed to fetch data for number")
        return ""

    soup = BeautifulSoup(response.text, features="html.parser")
    movie_list_div = soup.find('div', class_='movie-list h cols-4 vcols-8')
    if not movie_list_div:
        init.logger.info("No movies found or structure mismatch.")
        return

    item_divs = movie_list_div.findAll('div', class_='item')
    if not item_divs:
        init.logger.info("No items found in movie list.")
        return

    for item_div in item_divs:
        description = item_div.get_text()
        if (number in description or number.upper() in description) and ("å«ä¸­å­—ç£éˆ" in description or "å«ç£éˆ" in description):
            href = item_div.find('a', class_='box').get('href')
            # æ›´æ–°å‘å¸ƒurl
            update_pub_url(number, f"{base_url}{href}")
            magnet_link_list = crawl_magnet(f"{base_url}{href}")
            return magnet_link_list
    return None


def crawl_magnet(url):
    headers = {
        "user-agent": init.USER_AGENT,
        "cookie": init.JAVDB_COOKIE,
    }
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        init.logger.warn(f"Failed to fetch data for number")
        return ""
    
    magnet_link_list = []

    soup = BeautifulSoup(response.text, features="html.parser")
    magnet_div = soup.find('div', class_='magnet-links')
    
    item_columns_odd = magnet_div.findAll('div', class_='item columns is-desktop odd')
    for item_column_odd in item_columns_odd:
        score = 0.0
        magnet_link = item_column_odd.find('a').get('href')
        tags_div = item_column_odd.find('div', class_='tags')
        if tags_div is not None:
            for tag in tags_div.find_all('span'):
                if 'é«˜æ¸…' in tag.text:
                    score += init.bot_config['subscribe']['sub_weight']['hd']
                elif 'å­—å¹•' in tag.text:
                    score += init.bot_config['subscribe']['sub_weight']['subtitle_zh']
        date_div = item_column_odd.find('div', class_='date')
        date = date_div.find('span', class_='time').text
        score += calculate_score(date)
        magnet_link_list.append({"score": score, "magnet_link": magnet_link})

    item_columns = magnet_div.findAll('div', class_='item columns is-desktop')
    for item_column in item_columns:
        score = 0.0
        magnet_link = item_column.find('a').get('href')
        tags_div = item_column.find('div', class_='tags')
        if tags_div is not None:
            for tag in tags_div.find_all('span'):
                if 'é«˜æ¸…' in tag.text:
                    score += init.bot_config['subscribe']['sub_weight']['hd']
                elif 'å­—å¹•' in tag.text:
                    score += init.bot_config['subscribe']['sub_weight']['subtitle_zh']
        date_div = item_column.find('div', class_='date')
        date = date_div.find('span', class_='time').text
        score += calculate_score(date)
        magnet_link_list.append({"score": score, "magnet_link": magnet_link})
    # æŒ‰è¯„çº§ä»é«˜åˆ°ä½æ’åº
    sorted_res_list = sorted(magnet_link_list, key=lambda x: x['score'], reverse=True)
    return sorted_res_list


def days_since(date_str):
    # å°†è¾“å…¥æ—¥æœŸå­—ç¬¦ä¸²è§£æä¸ºæ—¥æœŸå¯¹è±¡
    input_date = datetime.strptime(date_str, "%Y-%m-%d")
    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = datetime.today()
    # è®¡ç®—å¤©æ•°å·®
    delta = today - input_date
    # è¿”å›å·®å€¼çš„å¤©æ•°
    return delta.days


def calculate_score(date_str):
    days = days_since(date_str)
    # ä½¿ç”¨è¯„åˆ†å…¬å¼ï¼š1 / (1 + days)
    return 1 / (1 + days)


# å®šæ—¶ä»»åŠ¡ï¼Œæ›´æ–°è®¢é˜…æ¼”å‘˜çš„è®¢é˜…åˆ—è¡¨
def schedule_actor():
    actor_list = get_actors()
    for actor in actor_list:
        add_subscribe2db(actor['actor_name'], actor['sub_user'])
        time.sleep(3)


# å®šæ—¶ä»»åŠ¡ï¼Œå®šæ—¶æŸ¥çœ‹å·²è®¢é˜…çš„æ¼”å‘˜æ˜¯å¦æœ‰æ›´æ–°
def schedule_number():
    with SqlLiteLib() as sqlite:
        try:
            # æŸ¥è¯¢éœ€è¦å¤„ç†çš„æ•°æ®
            query = "SELECT number, actor_name FROM subscribe WHERE is_download = 0"
            rows = sqlite.query(query)
            if not rows:
                init.logger.info("è®¢é˜…çš„è€å¸ˆè¿˜æœ¨æœ‰å‘å¸ƒæ–°ä½œå‘¦~")
                return
            for row in rows:
                number, actor_name = row
                magnet_link_list = get_magnet_by_number(number)
                if not magnet_link_list:  # æ£€æŸ¥æ˜¯å¦è¿”å›æœ‰æ•ˆç£åŠ›é“¾æ¥åˆ—è¡¨
                    init.logger.info(f"[{number}]çš„ç£åŠ›é“¾æ¥å°šæœªå‘å¸ƒ")
                    continue
                
                # ä¾æ¬¡ä¸‹è½½ï¼Œç›´åˆ°æˆåŠŸåé€€å‡º
                for item in magnet_link_list:
                    magnet_link = item['magnet_link']
                    init.logger.warn(f"å°è¯•ä½¿ç”¨[{magnet_link}]ç¦»çº¿åˆ°115ï¼Œè¯·ç¨å...")
                    # è‡ªåŠ¨æ·»åŠ åˆ°ç¦»çº¿ä¸‹è½½
                    if download2spec_path(magnet_link, number, actor_name):
                        # æ›´æ–°ä¸‹è½½çŠ¶æ€å’Œä¸‹è½½é“¾æ¥
                        update_download_sql = "UPDATE subscribe SET is_download = 1, magnet = ? WHERE number = ?"
                        sqlite.execute_sql(update_download_sql, (magnet_link, number))

                        # å‘é€æ¶ˆæ¯ç»™ç”¨æˆ·
                        send_message2usr(number, sqlite)
                        break
                    else:
                        init.logger.warn(f"[{magnet_link}]ç¦»çº¿å¤±è´¥ï¼Œç»§ç»­å°è¯•ä½¿ç”¨å…¶å®ƒç£åŠ›ä¸‹è½½...")
                # æ¯æ¬¡å¤„ç†å®Œä¸€ä¸ªä»»åŠ¡åç­‰å¾… 10 ç§’
                time.sleep(10)

        except Exception as e:
            # æ•è·å¹¶è®°å½•å¼‚å¸¸
            init.logger.warn(f"æ‰§è¡Œå®šæ—¶ä»»åŠ¡æ—¶ï¼Œå‡ºç°é”™è¯¯: {e}")

def send_message2usr(number, sqlite):
    try:
        query = "select sub_user,magnet,post_url,actor_name,score,title,pub_url from subscribe where number=?"
        params = (number,)
        res = sqlite.query(query, params)
        if not res:
            init.logger.warn(f"æœªæ‰¾åˆ°ç¼–å·ä¸º[{number}]çš„è®°å½•!")
            return
        sub_user = res[0][0]
        magnet = res[0][1]
        post_url = res[0][2]
        actor_name= res[0][3]
        score = res[0][4]
        title = res[0][5]
        pub_url = res[0][6]
        msg_title = escape_markdown_v2(f"[{number}] {title} è®¢é˜…å·²ä¸‹è½½!")
        msg_actor_name = escape_markdown_v2(actor_name)
        msg_score = escape_markdown_v2(str(score))
        message = f"""
                **{msg_title}**

                **æ¼”å‘˜:** {msg_actor_name}  
                **è¯„åˆ†:** {msg_score}  
                **ä¸‹è½½é“¾æ¥:** `{magnet}`  
                **å‘å¸ƒé“¾æ¥:** [ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…]({pub_url})
                """
        add_task_to_queue(sub_user, post_url, message)
        init.logger.info(f"[{number}] åŠ å…¥é˜Ÿåˆ—æˆåŠŸï¼")

    except Exception as e:
        init.logger.error(f"ç¼–å· [{number}] æ·»åŠ åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")
    

# async def queue_worker(loop, token):
#     global global_loop
#     """ åå°é˜Ÿåˆ—å¤„ç†ä»»åŠ¡ """
#     global_loop = loop
#     # bot
#     bot = Bot(token=token)
#     init.logger.info("æ¶ˆæ¯é˜Ÿåˆ—çº¿ç¨‹å¯åŠ¨æˆåŠŸï¼")
#     while True:
#         try:
#             # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
#             sub_user, post_url, message = await message_queue.get()
#             init.logger.info(f"å–å‡ºä»»åŠ¡: ç”¨æˆ·[{sub_user}], é“¾æ¥[{post_url}], æ¶ˆæ¯[{message}]")
#             # æ‰§è¡Œå‘é€
#             await bot.send_photo(
#                 chat_id=sub_user,
#                 photo=post_url,
#                 caption=message,
#                 parse_mode="MarkdownV2"
#             )
#             init.logger.info(f"æ¶ˆæ¯å·²å‘é€è‡³ {sub_user}")
#             # æ ‡è®°ä»»åŠ¡å®Œæˆ
#             message_queue.task_done()
#             # é—´éš”é˜²æ­¢é€Ÿç‡é™åˆ¶
#             await asyncio.sleep(3)
#         except Exception as e:
#             init.logger.error(f"é˜Ÿåˆ—ä»»åŠ¡å¤„ç†å¤±è´¥: {e}")


# def add_task_to_queue(sub_user, post_url, message):
#     """å‘æ¶ˆæ¯é˜Ÿåˆ—ä¸­æ·»åŠ ä»»åŠ¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
#     future = asyncio.run_coroutine_threadsafe(
#         message_queue.put((sub_user, post_url, message)),
#         global_loop 
#     )
#     try:
#         future.result(timeout=10)  # ç­‰å¾…ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´
#         init.logger.info(f"ä»»åŠ¡å·²æ·»åŠ åˆ°é˜Ÿåˆ—: {sub_user}, {post_url}, {message}")
#     except TimeoutError:
#         init.logger.error(f"æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—è¶…æ—¶: {sub_user}, {post_url}, {message}")
#     except Exception as e:
#         init.logger.error(f"æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")


def download2spec_path(magnet_link, number, actor_name):
    try: 
        save_path = f"{init.bot_config['subscribe']['path']}/{actor_name}"
        if not init.initialize_115client():
            init.logger.error(f"ğŸ’€115Cookieå·²è¿‡æœŸï¼Œè¯·é‡æ–°è®¾ç½®ï¼")
            return False
        # åˆ›å»ºç›®å½•
        init.client_115.create_folder(save_path)
        response, resource_name = init.client_115.offline_download(magnet_link)
        if response.get('errno') is not None:
            init.logger.error(f"âŒç¦»çº¿é‡åˆ°é”™è¯¯ï¼error_type: {response.get('errtype')}ï¼")
        else:
            init.logger.info(f"âœ…[{resource_name}]æ·»åŠ ç¦»çº¿æˆåŠŸ")
            download_success = init.client_115.check_offline_download_success(magnet_link, resource_name)
            if download_success:
                init.logger.info(f"âœ…[{resource_name}]ç¦»çº¿ä¸‹è½½å®Œæˆ")
                if init.client_115.is_directory(f"{init.bot_config['offline_path']}/{resource_name}"):
                    # æ¸…é™¤åƒåœ¾æ–‡ä»¶
                    init.client_115.auto_clean(f"{init.bot_config['offline_path']}/{resource_name}")
                    # é‡ååèµ„æº
                    init.client_115.rename(f"{init.bot_config['offline_path']}/{resource_name}", f"{init.bot_config['offline_path']}/{number}")
                    # ç§»åŠ¨æ–‡ä»¶
                    init.client_115.move_file(f"{init.bot_config['offline_path']}/{number}", save_path)
                else:
                    # åˆ›å»ºç•ªå·æ–‡ä»¶å¤¹
                    init.client_115.create_folder(f"{init.bot_config['offline_path']}/{number}")
                    # ç§»åŠ¨æ–‡ä»¶åˆ°ç•ªå·æ–‡ä»¶å¤¹
                    init.client_115.move_file(f"{init.bot_config['offline_path']}/{resource_name}", f"{init.bot_config['offline_path']}/{number}")
                    # ç§»åŠ¨ç•ªå·æ–‡ä»¶å¤¹åˆ°æŒ‡å®šç›®å½•
                    init.client_115.move_file(f"{init.bot_config['offline_path']}/{number}", save_path)
                
                # è¯»å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
                file_list = init.client_115.get_files_from_dir(f"{save_path}/{number}")
                # åˆ›å»ºè½¯é“¾
                create_strm_file(f"{save_path}/{number}", file_list)
                # é€šçŸ¥Embyæ‰«åº“
                notice_emby_scan_library()
                return True
            else:
                # ä¸‹è½½è¶…æ—¶åˆ é™¤ä»»åŠ¡
                init.client_115.clear_failed_task(magnet_link, resource_name)
                return False
    except Exception as e:
        init.logger.error(f"ğŸ’€ä¸‹è½½é‡åˆ°é”™è¯¯: {str(e)}")
        return False


def get_actors():
    with SqlLiteLib() as sqlite:
        sql = "select actor_name, sub_user from actor where is_delete=?"
        params = ("0",)
        result = sqlite.query(sql, params)
        return [{"actor_name": row[0], "sub_user": row[1]} for row in result]
    

def escape_markdown_v2(text: str) -> str:
    """
    è½¬ä¹‰å­—ç¬¦ä¸²ä»¥ç¬¦åˆ Telegram MarkdownV2 çš„è¦æ±‚ã€‚
    å¦‚æœå­—ç¬¦ä¸²è¢«åå¼•å·åŒ…è£¹ï¼Œåˆ™å†…éƒ¨å†…å®¹ä¸è½¬ä¹‰ã€‚
    :param text: åŸå§‹å­—ç¬¦ä¸²
    :return: è½¬ä¹‰åçš„å­—ç¬¦ä¸²
    """
    # éœ€è¦è½¬ä¹‰çš„å­—ç¬¦
    escape_chars = r"\_*[]()~`>#+-=|{}.!"

    # åˆ¤æ–­æ˜¯å¦è¢«åå¼•å·åŒ…è£¹
    if text.startswith("`") and text.endswith("`"):
        # åå¼•å·åŒ…è£¹çš„å†…å®¹ä¸è½¬ä¹‰
        return text
    else:
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        escaped_text = "".join(f"\\{char}" if char in escape_chars else char for char in text)
        return escaped_text
    


if __name__ == '__main__':
    init.init()
    magnet_link = get_magnet_by_number("OFJE-484")
    print(magnet_link)
    # number = "THU-043"
    # title = "å®Œå…¨ä¸»è¦³Ã—é¬¼ã‚¤ã‚«ã› 8æ™‚é–“BEST vol.01 éˆ´æ‘ã‚ã„ã‚Š æ²³åˆã‚ã™ãª é‡ã€…æµ¦æš– æ¶¼æ£®ã‚Œã‚€ å…«æ›ã†ã¿"
    # actor_name = "æ¶¼æ£®ç²å¤¢"
    # score = "0.0"
    # pub_url = "https://javdb.com/v/mOQN1r"
    # msg_title = escape_markdown_v2(f"[{number}] {title} è®¢é˜…å·²ä¸‹è½½!")
    # msg_actor_name = escape_markdown_v2(actor_name)
    # magnet = "magnet:?xt=urn:btih:57c7be25daec95af868a1be865442226c3385211&dn=[javdb.com]abf-208"
    # message = f"""
    #         **{msg_title}**

    #         **æ¼”å‘˜:** {msg_actor_name}  
    #         **è¯„åˆ†:** {score}  
    #         **ä¸‹è½½é“¾æ¥:** `{magnet}`  
    #         **å‘å¸ƒé“¾æ¥:** [ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…]({pub_url})
    #             """
    # print(message)
